{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "spark = SparkSession.builder.getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"weekly_boarding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField,StringType, IntegerType, DoubleType, LongType,DateType\n",
    "\n",
    "schema = StructType([StructField(\"TripID\", IntegerType(), True),\n",
    "                         StructField(\"RouteID\", StringType(), True),StructField(\"StopID\", IntegerType(), True),\n",
    "\t\t\t\tStructField(\"StopName\", StringType(), True),StructField(\"WeekBegining\", DateType(), True)\n",
    "\t\t\t\t,StructField(\"NumberOfBoardings\", IntegerType(), True),StructField(\"formatted_address\", StringType(), True)\n",
    "\t\t\t\t,StructField(\"latitude\", DoubleType(), True),StructField(\"longitude\", DoubleType(), True)\n",
    "\t\t\t\t,StructField(\"postcode\", StringType(), True),StructField(\"type\", StringType(), True)\n",
    "\t\t\t\t,StructField(\"route_desc\", StringType(), True),StructField(\"dist_from_centre\", DoubleType(), True),StructField(\"holiday_label\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import StructType, StructField,StringType, IntegerType, DoubleType, LongType,DateType\n",
    "\n",
    "schema = StructType([StructField(\"TripID\", IntegerType(), True),\n",
    "                         StructField(\"RouteID\", StringType(), True),StructField(\"StopID\", IntegerType(), True),\n",
    "\t\t\t\tStructField(\"StopName\", StringType(), True),StructField(\"WeekBegining\", DateType(), True)\n",
    "\t\t\t\t,StructField(\"NumberOfBoardings\", IntegerType(), True),StructField(\"formatted_address\", StringType(), True)\n",
    "\t\t\t\t,StructField(\"latitude\", DoubleType(), True),StructField(\"longitude\", DoubleType(), True)\n",
    "\t\t\t\t,StructField(\"postcode\", StringType(), True),StructField(\"type\", StringType(), True)\n",
    "\t\t\t\t,StructField(\"route_desc\", StringType(), True),StructField(\"dist_from_centre\", DoubleType(), True),StructField(\"holiday_label\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType().add(\"TripID\",IntegerType(),True).add(\"RouteID\",StringType(),True).add(\"StopID\",IntegerType(),True).add(\"StopName\",StringType(),True).add(\"WeekBegining\",DateType(),True).add(\"NumberOfBoardings\",IntegerType(),True).add(\"formatted_address\",StringType(),True).add(\"latitude\",DoubleType(),True).add(\"longitude\",DoubleType(),True).add(\"postcode\",StringType(),True).add(\"type\",StringType(),True).add(\"route_desc\",StringType(),True).add(\"dist_from_centre\",DoubleType(),True).add(\"holiday_label\",IntegerType(),True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_schema=spark.read.format(\"csv\").option(\"header\",\"True\").schema(schema).load(\"weekly_boarding.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "|TripID|RouteID|StopID|            StopName|WeekBegining|NumberOfBoardings|   formatted_address|           latitude|         longitude|postcode|           type|          route_desc|  dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "| 23631|    100| 14156|        181 Cross Rd|  2013-06-30|                1|181 Cross Rd, Wes...|        -34.9666565|       138.5921483|    5041| street_address|via Woodville Roa...| 5.180961380506887|            0|\n",
      "| 23631|    100| 14144|        177 Cross Rd|  2013-06-30|                1|177 Cross Rd, Wes...|        -34.9666071|       138.5923007|    5041| street_address|via Woodville Roa...| 5.172525305414759|            0|\n",
      "| 23632|    100| 14132|        175 Cross Rd|  2013-06-30|                1|175 Cross Rd, Wes...|-34.966757799999996|       138.5927151|    5041| street_address|via Woodville Roa...| 5.180708757110343|            0|\n",
      "| 23633|    100| 12266|Zone A Arndale In...|  2013-06-30|                2|Zone A Arndale In...|          -34.87516|        138.551628|    5009|transit_station|via Woodville Roa...| 7.057549100180063|            0|\n",
      "| 23633|    100| 14147|        178 Cross Rd|  2013-06-30|                1|178 Cross Rd, Mal...|        -34.9649596|        138.611477|    5061| street_address|via Woodville Roa...| 4.900099234428335|            0|\n",
      "| 23634|    100| 13907|       9A  Marion Rd|  2013-06-30|                1|9 Marion Rd, Torr...|        -34.9256552|       138.5523584|    5031| street_address|via Woodville Roa...| 4.807796587273396|            0|\n",
      "| 23634|    100| 14132|        175 Cross Rd|  2013-06-30|                1|175 Cross Rd, Wes...|-34.966757799999996|       138.5927151|    5041| street_address|via Woodville Roa...| 5.180708757110343|            0|\n",
      "| 23634|    100| 13335|    9A  Holbrooks Rd|  2013-06-30|                1|9 Holbrooks Rd, F...|        -34.9058902|       138.5511989|    5025| street_address|via Woodville Roa...| 5.178866284604099|            0|\n",
      "| 23634|    100| 13875|        9  Marion Rd|  2013-06-30|                1|9 Marion Rd, Torr...|        -34.9256552|       138.5523584|    5031| street_address|via Woodville Roa...| 4.807796587273396|            0|\n",
      "| 23634|    100| 13045|    206 Holbrooks Rd|  2013-06-30|                1|206 Holbrooks Rd,...|-34.922813500000004|       138.5484784|    5032| street_address|via Woodville Roa...| 5.139625006923029|            0|\n",
      "| 23635|    100| 13335|    9A  Holbrooks Rd|  2013-06-30|                1|9 Holbrooks Rd, F...|        -34.9058902|       138.5511989|    5025| street_address|via Woodville Roa...| 5.178866284604099|            0|\n",
      "| 23635|    100| 13383|       8A  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 13586|       8D  Marion Rd|  2013-06-30|                2|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 12726|       23  Findon Rd|  2013-06-30|                1|23 Findon Rd, Woo...|-34.882402299999995|       138.5312475|    5011|        premise|via Woodville Roa...| 7.980582863248004|            0|\n",
      "| 23635|    100| 13813|       8K  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 14062|        20  Cross Rd|  2013-06-30|                1|20 Cross Rd, Urrb...|        -34.9639244|138.63896709999997|    5064| street_address|via Woodville Roa...| 5.677892683112502|            0|\n",
      "| 23636|    100| 12780|  22A  Crittenden Rd|  2013-06-30|                1|22 Crittenden Rd,...|-34.903504600000005|       138.5449643|    5023| street_address|via Woodville Roa...| 5.803639984741648|            0|\n",
      "| 23636|    100| 13383|       8A  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23636|    100| 14154|        180 Cross Rd|  2013-06-30|                2|180 Cross Rd, Mal...|         -34.965057|       138.6109484|    5061| street_address|via Woodville Roa...|4.9051084510922465|            0|\n",
      "| 23636|    100| 13524|       8C  Marion Rd|  2013-06-30|                3|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TripID: integer (nullable = true)\n",
      " |-- RouteID: string (nullable = true)\n",
      " |-- StopID: integer (nullable = true)\n",
      " |-- StopName: string (nullable = true)\n",
      " |-- WeekBegining: date (nullable = true)\n",
      " |-- NumberOfBoardings: integer (nullable = true)\n",
      " |-- formatted_address: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- route_desc: string (nullable = true)\n",
      " |-- dist_from_centre: double (nullable = true)\n",
      " |-- holiday_label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pyspark\\sql\\dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.registerTempTable(\"bus_boarding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10857234"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_schema.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_with_schema.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TripID',\n",
       " 'RouteID',\n",
       " 'StopID',\n",
       " 'StopName',\n",
       " 'WeekBegining',\n",
       " 'NumberOfBoardings',\n",
       " 'formatted_address',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'postcode',\n",
       " 'type',\n",
       " 'route_desc',\n",
       " 'dist_from_centre',\n",
       " 'holiday_label']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_schema.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|TripID|RouteID|StopID|StopName|WeekBegining|NumberOfBoardings|formatted_address|latitude|longitude|postcode|type|route_desc|dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|     0|      0|     0|       0|           0|                0|             3506|       0|        0|  425081|   0|   2106618|               0|            0|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_with_schema.select(*[\n",
    "    (\n",
    "        F.count(F.when((F.isnan(c) | F.col(c).isNull()), c)) if t not in (\"timestamp\", \"date\")\n",
    "        else F.count(F.when(F.col(c).isNull(), c))\n",
    "    ).alias(c)\n",
    "    for c, t in df_with_schema.dtypes if c in df_with_schema.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_with_schema=df_with_schema.na.fill(\"unknown\",[\"formatted_address\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|TripID|RouteID|StopID|StopName|WeekBegining|NumberOfBoardings|formatted_address|latitude|longitude|postcode|type|route_desc|dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_with_schema.filter(df_with_schema.formatted_address.isNull()).show()\n",
    "#df_with_schema.filter(\"formatted_address is NULL\").show()\n",
    "#df_with_schema.filter(col(\"formatted_address\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_schema=df_with_schema.na.fill(\"unknown\",[\"route_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|TripID|RouteID|StopID|StopName|WeekBegining|NumberOfBoardings|formatted_address|latitude|longitude|postcode|type|route_desc|dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.filter(df_with_schema.route_desc.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|TripID|RouteID|StopID|StopName|WeekBegining|NumberOfBoardings|formatted_address|latitude|longitude|postcode|type|route_desc|dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|     0|      0|     0|       0|           0|                0|                0|       0|        0|  425081|   0|         0|               0|            0|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_with_schema.select(*[\n",
    "    (\n",
    "        F.count(F.when((F.isnan(c) | F.col(c).isNull()), c)) if t not in (\"timestamp\", \"date\")\n",
    "        else F.count(F.when(F.col(c).isNull(), c))\n",
    "    ).alias(c)\n",
    "    for c, t in df_with_schema.dtypes if c in df_with_schema.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+-----+\n",
      "|TripID|RouteID|StopID|StopName|WeekBegining|NumberOfBoardings|formatted_address|latitude|longitude|postcode|type|route_desc|dist_from_centre|holiday_label|count|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+-----+\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_duplicates = df_with_schema.groupBy(df_with_schema.columns).count().filter(col(\"count\")>1)\n",
    "df_duplicates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10857234"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_schema.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_schema=df_with_schema.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "|TripID|RouteID|StopID|            StopName|WeekBegining|NumberOfBoardings|   formatted_address|           latitude|         longitude|postcode|           type|          route_desc|  dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "| 23631|    100| 14156|        181 Cross Rd|  2013-06-30|                1|181 Cross Rd, Wes...|        -34.9666565|       138.5921483|    5041| street_address|via Woodville Roa...| 5.180961380506887|            0|\n",
      "| 23631|    100| 14144|        177 Cross Rd|  2013-06-30|                1|177 Cross Rd, Wes...|        -34.9666071|       138.5923007|    5041| street_address|via Woodville Roa...| 5.172525305414759|            0|\n",
      "| 23632|    100| 14132|        175 Cross Rd|  2013-06-30|                1|175 Cross Rd, Wes...|-34.966757799999996|       138.5927151|    5041| street_address|via Woodville Roa...| 5.180708757110343|            0|\n",
      "| 23633|    100| 12266|Zone A Arndale In...|  2013-06-30|                2|Zone A Arndale In...|          -34.87516|        138.551628|    5009|transit_station|via Woodville Roa...| 7.057549100180063|            0|\n",
      "| 23633|    100| 14147|        178 Cross Rd|  2013-06-30|                1|178 Cross Rd, Mal...|        -34.9649596|        138.611477|    5061| street_address|via Woodville Roa...| 4.900099234428335|            0|\n",
      "| 23634|    100| 13907|       9A  Marion Rd|  2013-06-30|                1|9 Marion Rd, Torr...|        -34.9256552|       138.5523584|    5031| street_address|via Woodville Roa...| 4.807796587273396|            0|\n",
      "| 23634|    100| 14132|        175 Cross Rd|  2013-06-30|                1|175 Cross Rd, Wes...|-34.966757799999996|       138.5927151|    5041| street_address|via Woodville Roa...| 5.180708757110343|            0|\n",
      "| 23634|    100| 13335|    9A  Holbrooks Rd|  2013-06-30|                1|9 Holbrooks Rd, F...|        -34.9058902|       138.5511989|    5025| street_address|via Woodville Roa...| 5.178866284604099|            0|\n",
      "| 23634|    100| 13875|        9  Marion Rd|  2013-06-30|                1|9 Marion Rd, Torr...|        -34.9256552|       138.5523584|    5031| street_address|via Woodville Roa...| 4.807796587273396|            0|\n",
      "| 23634|    100| 13045|    206 Holbrooks Rd|  2013-06-30|                1|206 Holbrooks Rd,...|-34.922813500000004|       138.5484784|    5032| street_address|via Woodville Roa...| 5.139625006923029|            0|\n",
      "| 23635|    100| 13335|    9A  Holbrooks Rd|  2013-06-30|                1|9 Holbrooks Rd, F...|        -34.9058902|       138.5511989|    5025| street_address|via Woodville Roa...| 5.178866284604099|            0|\n",
      "| 23635|    100| 13383|       8A  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 13586|       8D  Marion Rd|  2013-06-30|                2|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 12726|       23  Findon Rd|  2013-06-30|                1|23 Findon Rd, Woo...|-34.882402299999995|       138.5312475|    5011|        premise|via Woodville Roa...| 7.980582863248004|            0|\n",
      "| 23635|    100| 13813|       8K  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 14062|        20  Cross Rd|  2013-06-30|                1|20 Cross Rd, Urrb...|        -34.9639244|138.63896709999997|    5064| street_address|via Woodville Roa...| 5.677892683112502|            0|\n",
      "| 23636|    100| 12780|  22A  Crittenden Rd|  2013-06-30|                1|22 Crittenden Rd,...|-34.903504600000005|       138.5449643|    5023| street_address|via Woodville Roa...| 5.803639984741648|            0|\n",
      "| 23636|    100| 13383|       8A  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23636|    100| 14154|        180 Cross Rd|  2013-06-30|                2|180 Cross Rd, Mal...|         -34.965057|       138.6109484|    5061| street_address|via Woodville Roa...|4.9051084510922465|            0|\n",
      "| 23636|    100| 13524|       8C  Marion Rd|  2013-06-30|                3|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10432153"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_schema.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425081"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10857234-10432153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|TripID|RouteID|StopID|StopName|WeekBegining|NumberOfBoardings|formatted_address|latitude|longitude|postcode|type|route_desc|dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|     0|      0|     0|       0|           0|                0|                0|       0|        0|       0|   0|         0|               0|            0|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_with_schema.select(*[\n",
    "    (\n",
    "        F.count(F.when((F.isnan(c) | F.col(c).isNull()), c)) if t not in (\"timestamp\", \"date\")\n",
    "        else F.count(F.when(F.col(c).isNull(), c))\n",
    "    ).alias(c)\n",
    "    for c, t in df_with_schema.dtypes if c in df_with_schema.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_max=df_with_schema.groupBy('StopName',\"WeekBegining\",'type').max(\"NumberOfBoardings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+--------------+----------------------+\n",
      "|        StopName|WeekBegining|          type|max(NumberOfBoardings)|\n",
      "+----------------+------------+--------------+----------------------+\n",
      "|  11A  Marion Rd|  2013-08-04|street_address|                    44|\n",
      "|   23  Findon Rd|  2013-08-04|       premise|                    44|\n",
      "|    169 Cross Rd|  2013-08-11|street_address|                    14|\n",
      "|   17  Grange Rd|  2013-08-18|street_address|                    20|\n",
      "|224 Woodville Rd|  2013-08-18|street_address|                    22|\n",
      "+----------------+------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_max.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_sum=df_with_schema.groupBy('StopName',\"WeekBegining\",'type').sum(\"NumberOfBoardings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+--------------+----------------------+\n",
      "|        StopName|WeekBegining|          type|sum(NumberOfBoardings)|\n",
      "+----------------+------------+--------------+----------------------+\n",
      "|  11A  Marion Rd|  2013-08-04|street_address|                  1423|\n",
      "|   23  Findon Rd|  2013-08-04|       premise|                  1196|\n",
      "|    169 Cross Rd|  2013-08-11|street_address|                    85|\n",
      "|   17  Grange Rd|  2013-08-18|street_address|                   864|\n",
      "|224 Woodville Rd|  2013-08-18|street_address|                    87|\n",
      "+----------------+------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_sum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_min=df_with_schema.groupBy('StopName',\"WeekBegining\",'type').min(\"NumberOfBoardings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+--------------+----------------------+\n",
      "|        StopName|WeekBegining|          type|min(NumberOfBoardings)|\n",
      "+----------------+------------+--------------+----------------------+\n",
      "|  11A  Marion Rd|  2013-08-04|street_address|                     1|\n",
      "|   23  Findon Rd|  2013-08-04|       premise|                     1|\n",
      "|    169 Cross Rd|  2013-08-11|street_address|                     1|\n",
      "|   17  Grange Rd|  2013-08-18|street_address|                     1|\n",
      "|224 Woodville Rd|  2013-08-18|street_address|                     1|\n",
      "+----------------+------------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_min.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TripID: integer (nullable = true)\n",
      " |-- RouteID: string (nullable = true)\n",
      " |-- StopID: integer (nullable = true)\n",
      " |-- StopName: string (nullable = true)\n",
      " |-- WeekBegining: date (nullable = true)\n",
      " |-- NumberOfBoardings: integer (nullable = true)\n",
      " |-- formatted_address: string (nullable = false)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- route_desc: string (nullable = false)\n",
      " |-- dist_from_centre: double (nullable = true)\n",
      " |-- holiday_label: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('TripID', 'int'),\n",
       " ('RouteID', 'string'),\n",
       " ('StopID', 'int'),\n",
       " ('StopName', 'string'),\n",
       " ('WeekBegining', 'date'),\n",
       " ('NumberOfBoardings', 'int'),\n",
       " ('formatted_address', 'string'),\n",
       " ('latitude', 'double'),\n",
       " ('longitude', 'double'),\n",
       " ('postcode', 'string'),\n",
       " ('type', 'string'),\n",
       " ('route_desc', 'string'),\n",
       " ('dist_from_centre', 'double'),\n",
       " ('holiday_label', 'int')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a row count\n",
    "df_with_schema.count()\n",
    "\n",
    "# print the schema (shape of your df)\n",
    "df_with_schema.printSchema()\n",
    " \n",
    "# get the columns as a list\n",
    "df_with_schema.columns\n",
    " \n",
    "# get the columns and types as tuples in a list\n",
    "df_with_schema.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10432153"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a row count\n",
    "df_with_schema.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_with_schema.groupby('StopName','WeekBeginning','type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|              type|  count|\n",
      "+------------------+-------+\n",
      "|   transit_station|1604089|\n",
      "|           premise|1307928|\n",
      "| point_of_interest|  46607|\n",
      "|            school|   5248|\n",
      "|      intersection|  34142|\n",
      "|         political|  11250|\n",
      "|    street_address|6650055|\n",
      "|             route| 765559|\n",
      "|        subpremise|   5892|\n",
      "|        university|    470|\n",
      "|real_estate_agency|    525|\n",
      "|     travel_agency|     21|\n",
      "|       supermarket|     57|\n",
      "|       post_office|      3|\n",
      "|             store|      4|\n",
      "|        restaurant|    303|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# assume `df` is your Spark DataFrame\n",
    "df_with_schema.groupBy('type').agg(count('type').alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum, max, min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, max, min\n",
    "\n",
    "grouped = df_with_schema.groupBy('StopName', 'WeekBegining', 'type').agg(sum('NumberOfBoardings').alias('sum'),\n",
    "                 count('NumberOfBoardings').alias('count'),\n",
    "                 max('NumberOfBoardings').alias('max'),\n",
    "                 min('NumberOfBoardings').alias('min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = grouped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StopName', 'WeekBegining', 'type', 'sum', 'count', 'max', 'min']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df_with_schema.count()\n",
    "num_cols = len(df_with_schema.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10432153"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = grouped.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198660"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pandas-to-spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+--------------+----+-----+---+---+\n",
      "|        StopName|WeekBegining|          type| sum|count|max|min|\n",
      "+----------------+------------+--------------+----+-----+---+---+\n",
      "|  11A  Marion Rd|  2013-08-04|street_address|1423|  314| 44|  1|\n",
      "|   23  Findon Rd|  2013-08-04|       premise|1196|  271| 44|  1|\n",
      "|    169 Cross Rd|  2013-08-11|street_address|  85|   32| 14|  1|\n",
      "|   17  Grange Rd|  2013-08-18|street_address| 864|  281| 20|  1|\n",
      "|224 Woodville Rd|  2013-08-18|street_address|  87|   31| 22|  1|\n",
      "+----------------+------------+--------------+----+-----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "st_week_grp1 = (grouped.groupBy(\"StopName\")\n",
    "                        .agg(count(\"WeekBegining\").alias(\"count\"))\n",
    "                        .select(\"StopName\", \"count\")\n",
    "                        .orderBy(col(\"count\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|          StopName|count|\n",
      "+------------------+-----+\n",
      "|      19 Marion Rd|   54|\n",
      "|29 Tapleys Hill Rd|   54|\n",
      "|      7 The Parade|   54|\n",
      "|        26 Port Rd|   54|\n",
      "|    U1 Grenfell St|   54|\n",
      "+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "st_week_grp1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "aa = st_week_grp1.filter(st_week_grp1['count'] == 54) \\\n",
    "                 .groupBy() \\\n",
    "                 .agg(collect_list('StopName').alias('StopNameList')) \\\n",
    "                 .collect()[0]['StopNameList']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29 Tapleys Hill Rd',\n",
       " '7 The Parade',\n",
       " '26 Port Rd',\n",
       " 'U1 Grenfell St',\n",
       " '36 Grand Junction Rd',\n",
       " '19 Princes Rd',\n",
       " '3 Richmond Rd',\n",
       " '34C RM Williams Dr',\n",
       " '36 Bridge Rd',\n",
       " '13 Main North Rd',\n",
       " '11 Hawker St',\n",
       " '38 Dunrobin Rd',\n",
       " '10 Ashley St',\n",
       " '38 Brighton Rd',\n",
       " '56 Military Rd',\n",
       " '77F Uley Rd',\n",
       " '58 Saints Rd',\n",
       " '31 Gorge Rd',\n",
       " '51 Sandison Rd']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|count|WeekBegining|\n",
      "+-----+------------+\n",
      "|   29|           5|\n",
      "|   26|          10|\n",
      "|   54|        3132|\n",
      "|   19|           5|\n",
      "|   22|           8|\n",
      "|    7|           8|\n",
      "|   34|          16|\n",
      "|   50|          27|\n",
      "|   32|          12|\n",
      "|   43|          10|\n",
      "|   31|           9|\n",
      "|   39|          10|\n",
      "|   25|           3|\n",
      "|    6|           9|\n",
      "|    9|          13|\n",
      "|   27|          13|\n",
      "|   51|          39|\n",
      "|   52|          69|\n",
      "|   17|          10|\n",
      "|   41|          37|\n",
      "+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "counts = st_week_grp1.groupBy('count').agg(count('*').alias('WeekBegining'))\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "bb = df_with_schema.filter(col(\"StopName\").isin(aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter rows where the 'StopName' column is in the list of stop names\n",
    "new_data = df_with_schema.filter(col('StopName').isin(aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in filtered_data: 9992660\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "filtered_data = new_data.filter(col('dist_from_centre') <= 100)\n",
    "# Alternatively, you can use the where() method:\n",
    "# filtered_data = new_data.where(col('dist_from_centre') <= 100)\n",
    "\n",
    "row_count = filtered_data.count()\n",
    "print(\"Number of rows in filtered_data:\", row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|            StopName|sum_of_boardings|\n",
      "+--------------------+----------------+\n",
      "|        19 Marion Rd|           29980|\n",
      "|  29 Tapleys Hill Rd|            7282|\n",
      "|      U1 Grenfell St|           99898|\n",
      "|36 Grand Junction Rd|           18233|\n",
      "|        7 The Parade|           36328|\n",
      "|          26 Port Rd|            4001|\n",
      "|Zone A Arndale In...|           72147|\n",
      "|       10  Marion Rd|            7618|\n",
      "|  33A West Lakes Bvd|            5475|\n",
      "|    13  Holbrooks Rd|           15131|\n",
      "|        Y1 Currie St|           71560|\n",
      "|        3 Dulwich Rd|            5596|\n",
      "|      U1 Victoria Sq|          163863|\n",
      "|        164 Cross Rd|            1427|\n",
      "|      45 Fletcher Rd|            7870|\n",
      "|   21  Crittenden Rd|           13438|\n",
      "|        168 Cross Rd|            4443|\n",
      "|      16 Portrush Rd|            3310|\n",
      "|      8 Greenhill Rd|           13079|\n",
      "|     18  Portrush Rd|            3115|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "stopageName_with_boarding = bb.groupBy('StopName') \\\n",
    "                              .agg(sum('NumberOfBoardings').alias('sum_of_boardings'))\n",
    "\n",
    "stopageName_with_boarding.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------------+\n",
      "|            StopName|Total_boarding_on_the_stopage|\n",
      "+--------------------+-----------------------------+\n",
      "|        19 Marion Rd|                        29980|\n",
      "|  29 Tapleys Hill Rd|                         7282|\n",
      "|      U1 Grenfell St|                        99898|\n",
      "|36 Grand Junction Rd|                        18233|\n",
      "|        7 The Parade|                        36328|\n",
      "|          26 Port Rd|                         4001|\n",
      "|Zone A Arndale In...|                        72147|\n",
      "|       10  Marion Rd|                         7618|\n",
      "|  33A West Lakes Bvd|                         5475|\n",
      "|    13  Holbrooks Rd|                        15131|\n",
      "|        Y1 Currie St|                        71560|\n",
      "|        3 Dulwich Rd|                         5596|\n",
      "|      U1 Victoria Sq|                       163863|\n",
      "|        164 Cross Rd|                         1427|\n",
      "|      45 Fletcher Rd|                         7870|\n",
      "|   21  Crittenden Rd|                        13438|\n",
      "|        168 Cross Rd|                         4443|\n",
      "|      16 Portrush Rd|                         3310|\n",
      "|      8 Greenhill Rd|                        13079|\n",
      "|     18  Portrush Rd|                         3115|\n",
      "+--------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopageName_with_boarding = stopageName_with_boarding \\\n",
    "                             .withColumnRenamed('StopName', 'StopName') \\\n",
    "                             .withColumnRenamed('sum_of_boardings', 'Total_boarding_on_the_stopage')\n",
    "\n",
    "stopageName_with_boarding.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|TripID|RouteID|StopID|StopName|NumberOfBoardings|formatted_address|latitude|longitude|postcode|type|route_desc|dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|     0|      0|     0|       0|                0|                0|       0|        0|       0|   0|         0|               0|            0|\n",
      "+------+-------+------+--------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find Count of Null, None, NaN of All DataFrame Columns\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "df_Columns=['TripID','RouteID','StopID','StopName','NumberOfBoardings','formatted_address','latitude','longitude','postcode','type','route_desc','dist_from_centre','holiday_label']\n",
    "df_with_schema.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_Columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TripID',\n",
       " 'RouteID',\n",
       " 'StopID',\n",
       " 'StopName',\n",
       " 'WeekBegining',\n",
       " 'NumberOfBoardings',\n",
       " 'formatted_address',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'postcode',\n",
       " 'type',\n",
       " 'route_desc',\n",
       " 'dist_from_centre',\n",
       " 'holiday_label']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_schema.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling Null values with others\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df_with_schema.withColumn('route_desc', when(col('route_desc').isNull(), 'others').otherwise(col('route_desc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[TripID: int, RouteID: string, StopID: int, StopName: string, WeekBegining: date, NumberOfBoardings: int, formatted_address: string, latitude: double, longitude: double, postcode: string, type: string, route_desc: string, dist_from_centre: double, holiday_label: int]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|          route_desc|  count|\n",
      "+--------------------+-------+\n",
      "|             unknown|1974222|\n",
      "|via Flinders Univ...| 356917|\n",
      "|via Military Road...| 322001|\n",
      "|via City & O-bahn...| 307832|\n",
      "|via Main North Ro...| 229590|\n",
      "|via Regency Road,...| 227401|\n",
      "|via St Bernards R...| 205820|\n",
      "|via Para Hills, P...| 141212|\n",
      "|via Sturt Road, B...| 136312|\n",
      "|via Mawson Interc...| 136077|\n",
      "|via Northgate & G...| 118235|\n",
      "|via Addison Road ...| 117469|\n",
      "|via Pierson Stree...| 115915|\n",
      "|via Lyell McEwin ...| 112658|\n",
      "|via Plympton Park...| 107806|\n",
      "|via Liberty Grove...| 105275|\n",
      "|via Main North Ro...| 103954|\n",
      "|via Marion Centre...| 102146|\n",
      "|via Salisbury Int...|  98991|\n",
      "|via Newcastle Str...|  98159|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To count the occurrences of each value in the route_desc column of a PySpark DataFrame, you can use the groupBy() and count() methods as follows:\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "route_desc_counts = df_with_schema.groupBy('route_desc').count().orderBy(desc('count'))\n",
    "\n",
    "route_desc_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnull, sum\n",
    "\n",
    "null_count = df_with_schema.select(sum(isnull(col('TripID')).cast('integer')).alias('TripID')).collect()[0]['TripID']\n",
    "\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a list comprehension to create a list of Column objects that count the number of null values in each column of the DataFrame df. We use the isnull() function to check for null values in each column, and the alias() method to give each column a name that matches its original name.\n",
    "\n",
    "\n",
    "We then use the agg() method with the unpacking operator (*) to aggregate the counts of null values for each column in the DataFrame df. The resulting DataFrame has one row and one column for each column in df.\n",
    "\n",
    "\n",
    "Finally, we use the show() method to display the resulting DataFrame, which shows the count of null values in each column of df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TripID: integer (nullable = true)\n",
      " |-- RouteID: string (nullable = true)\n",
      " |-- StopID: integer (nullable = true)\n",
      " |-- StopName: string (nullable = true)\n",
      " |-- WeekBegining: date (nullable = true)\n",
      " |-- NumberOfBoardings: integer (nullable = true)\n",
      " |-- formatted_address: string (nullable = false)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- route_desc: string (nullable = false)\n",
      " |-- dist_from_centre: double (nullable = true)\n",
      " |-- holiday_label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In PySpark, you can use the printSchema() method of a DataFrame to display its schema, which includes the column names and data types, as well as any nested structures or arrays.\n",
    "\n",
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|   formatted_address| count|\n",
      "+--------------------+------+\n",
      "|Currie St, Adelai...|228514|\n",
      "|King William Stre...|130862|\n",
      "|Pulteney St, Adel...|103164|\n",
      "|Grote St, Adelaid...| 79589|\n",
      "|Grenfell St, Adel...| 77575|\n",
      "|Zone C Paradise I...| 48144|\n",
      "|2 King William Rd...| 46398|\n",
      "|8 Marion Rd, Unde...| 40372|\n",
      "|Hutt St, Adelaide...| 37358|\n",
      "|Stop G3 Grenfell ...| 36992|\n",
      "|North Terrace & R...| 33800|\n",
      "|33 Mawson Lakes B...| 33434|\n",
      "|Stop X2 King Will...| 30427|\n",
      "|Zone E Marion Int...| 30326|\n",
      "|29 Sturt Rd, Brig...| 28178|\n",
      "|Stop D1 King Will...| 26310|\n",
      "|13 Payneham Rd, N...| 25755|\n",
      "|5 Sir Donald Brad...| 25739|\n",
      "|Klemzig SA 5087, ...| 25256|\n",
      "|Stop U1 Victoria ...| 25166|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Here, we use the groupBy() method to group the DataFrame df by the formatted_address column. We then use the count() method to count the occurrences of each value in the formatted_address column. We use the orderBy() method to sort the resulting DataFrame in descending order by the count of each value.\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "address_counts = df_with_schema.groupBy('formatted_address').count().orderBy(desc('count'))\n",
    "\n",
    "address_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling Null values with others\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df_with_schema.withColumn('formatted_address', when(col('formatted_address').isNull(), 'others').otherwise(col('formatted_address')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnull, sum\n",
    "\n",
    "null_count = df_with_schema.select(sum(isnull(col('formatted_address')).cast('integer')).alias('formatted_address')).collect()[0]['formatted_address']\n",
    "\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|postcode|  count|\n",
      "+--------+-------+\n",
      "|    5000|1417151|\n",
      "|    5082| 332974|\n",
      "|    5067| 324162|\n",
      "|    5007| 309843|\n",
      "|    5031| 292906|\n",
      "|    5035| 290194|\n",
      "|    5081| 285879|\n",
      "|    5108| 259000|\n",
      "|    5069| 242666|\n",
      "|    5034| 241610|\n",
      "|    5092| 220311|\n",
      "|    5112| 206618|\n",
      "|    5006| 175861|\n",
      "|    5014| 173883|\n",
      "|    5042| 173641|\n",
      "|    5022| 156166|\n",
      "|    5075| 152886|\n",
      "|    5045| 147144|\n",
      "|    5095| 140826|\n",
      "|    5087| 137284|\n",
      "+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "postcode_counts = df_with_schema.groupBy('postcode').count().orderBy(desc('count'))\n",
    "\n",
    "postcode_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dropna() method in PySpark is used to remove any rows that contain null or missing values. The resulting DataFrame only contains rows with complete data.\n",
    "cleaned = df_with_schema.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnull, sum\n",
    "\n",
    "null_count = df_with_schema.select(sum(isnull(col('postcode')).cast('integer')).alias('postcode')).collect()[0]['postcode']\n",
    "\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 10432153\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "#In PySpark, the DataFrame class does not have a shape attribute like in Pandas. However, you can use the count() and columns methods to get the number of rows and columns in the DataFrame cleaned as follows:\n",
    "\n",
    "num_rows = cleaned.count()\n",
    "num_cols = len(cleaned.columns)\n",
    "\n",
    "print(\"Number of rows: {}\".format(num_rows))\n",
    "print(\"Number of columns: {}\".format(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425081"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10857234-10432153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|TripID|RouteID|StopID|StopName|WeekBegining|NumberOfBoardings|formatted_address|latitude|longitude|postcode|type|route_desc|dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "|     0|      0|     0|       0|           0|                0|                0|       0|        0|       0|   0|         0|               0|            0|\n",
      "+------+-------+------+--------+------------+-----------------+-----------------+--------+---------+--------+----+----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In PySpark, you can use the isNull() method to check for null or missing values in a DataFrame. You can then use the sum() method to count the number of null values in each column of the DataFrame.\n",
    "from pyspark.sql.functions import col, sum as pyspark_sum\n",
    "\n",
    "null_counts = cleaned.select([pyspark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in cleaned.columns])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_agg = (grouped.groupBy('StopName')\n",
    "           .agg(F.sum('sum').alias('Boarding_sum'),\n",
    "                F.sum('count').alias('Boarding_count'),\n",
    "                F.max('max').alias('Boarding_max'))\n",
    "           .withColumn('Boarding_sum', F.col('Boarding_sum') / 54)\n",
    "           .withColumn('Boarding_count', F.col('Boarding_count') / 54)\n",
    "           .withColumn('Boarding_max', F.col('Boarding_max') / 54)\n",
    "           .orderBy('StopName'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the groupBy() method to group the DataFrame bb by the StopName column, and the agg() method to calculate the sum of the percentage change for each aggregation using the sum() and pct_change() functions from the pyspark.sql.functions module.\n",
    "\n",
    "\n",
    "We then use the withColumn() method to divide each column by 54 to get the average percentage change, and the orderBy() method to sort the resulting DataFrame by the StopName column.\n",
    "\n",
    "\n",
    "Finally, we use the toPandas() method to convert the resulting DataFrame d to a Pandas DataFrame pct_chng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------------------+------------------+\n",
      "|      StopName|      Boarding_sum|    Boarding_count|      Boarding_max|\n",
      "+--------------+------------------+------------------+------------------+\n",
      "|   1 Anzac Hwy| 730.1666666666666|  306.037037037037| 2.925925925925926|\n",
      "|  1 Botanic Rd| 275.3333333333333|173.12962962962962|0.7592592592592593|\n",
      "|    1 Frome Rd|1249.2222222222222| 159.7037037037037|1.8333333333333333|\n",
      "|1 Fullarton Rd|10.833333333333334| 7.092592592592593|0.2962962962962963|\n",
      "|   1 George St| 94.03703703703704| 43.27777777777778|0.5370370370370371|\n",
      "+--------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the top 5 largest values in the Boarding_sum column of the pct_chng DataFrame using PySpark, you can use the orderBy() and limit() methods as follows:\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "\n",
    "top_5 = pct_chng.sort_values(F.col('sum').asc()).limit(5)\n",
    "\n",
    "top_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------+------------+\n",
      "|StopName|Boarding_sum|Boarding_count|Boarding_max|\n",
      "+--------+------------+--------------+------------+\n",
      "+--------+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here, we use the limit() method to select the top 3111 rows from the pct_chng DataFrame,and the collect() method to return them as a list of Row objects. \n",
    "# We then use list comprehension to select the desired rows based on their positions, and create a new DataFrame selected_df using the createDataFrame() method.\n",
    "# Finally, we use the show() method to display the resulting DataFrame selected_df.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "indices = [3110, 2134, 214, 1538, 1290]\n",
    "selected_df = df_agg.filter(col(\"StopName\").isin(indices))\n",
    "\n",
    "selected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StopName: string (nullable = true)\n",
      " |-- Boarding_sum: double (nullable = true)\n",
      " |-- Boarding_count: double (nullable = true)\n",
      " |-- Boarding_max: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_geo = spark.read.csv(\"output_geo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_geo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField,StringType, IntegerType, DoubleType, LongType,DateType\n",
    "\n",
    "schema = StructType([StructField(\"accuracy\",  StringType(), True),\n",
    "                         StructField(\"formatted_address\", StringType(), True),StructField(\"google_place_id\",  StringType(), True),\n",
    "\t\t\t\tStructField(\"input_string\", StringType(), True)\n",
    "\t\t\t\t,StructField(\"latitude\", DoubleType(), True),StructField(\"longitude\", DoubleType(), True),StructField(\"number_of_results\", IntegerType(), True)\n",
    "\t\t\t\t,StructField(\"postcode\", StringType(), True),StructField(\"status\", StringType(), True)\n",
    "\t\t\t\t,StructField(\"type\", StringType(), True),StructField(\"dist_from_centre\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_geo_schema=spark.read.format(\"csv\").option(\"header\",\"True\").schema(schema).load(\"output_geo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+-----------+-----------+-----------------+--------+------+--------------------+----------------+\n",
      "|          accuracy|   formatted_address|     google_place_id|        input_string|   latitude|  longitude|number_of_results|postcode|status|                type|dist_from_centre|\n",
      "+------------------+--------------------+--------------------+--------------------+-----------+-----------+-----------------+--------+------+--------------------+----------------+\n",
      "|           ROOFTOP|181 Cross Rd, Wes...|ChIJKT7I9rbPsGoRV...|        181 Cross Rd|-34.9666565|138.5921483|                1|    5041|    OK|      street_address|            null|\n",
      "|           ROOFTOP|177 Cross Rd, Wes...|ChIJ-VFZ87bPsGoRy...|        177 Cross Rd|-34.9666071|138.5923007|                1|    5041|    OK|      street_address|            null|\n",
      "|           ROOFTOP|175 Cross Rd, Wes...|ChIJIztlirbPsGoR3...|        175 Cross Rd|-34.9667578|138.5927151|                1|    5041|    OK|      street_address|            null|\n",
      "|  GEOMETRIC_CENTER|Zone A Arndale In...|ChIJn0C1hCPGsGoRI...|Zone A Arndale In...|  -34.87516| 138.551628|                1|    5009|    OK|bus_station,estab...|            null|\n",
      "|           ROOFTOP|178 Cross Rd, Mal...|ChIJycNiylvOsGoRd...|        178 Cross Rd|-34.9649596| 138.611477|                1|    5061|    OK|      street_address|            null|\n",
      "|           ROOFTOP|9 Marion Rd, Torr...|ChIJjzH6jIXFsGoRC...|       9A  Marion Rd|-34.9256552|138.5523584|                1|    5031|    OK|      street_address|            null|\n",
      "|           ROOFTOP|9 Holbrooks Rd, F...|ChIJRxAIiPrFsGoRH...|    9A  Holbrooks Rd|-34.9058902|138.5511989|                1|    5025|    OK|      street_address|            null|\n",
      "|           ROOFTOP|9 Marion Rd, Torr...|ChIJjzH6jIXFsGoRC...|        9  Marion Rd|-34.9256552|138.5523584|                1|    5031|    OK|      street_address|            null|\n",
      "|           ROOFTOP|206 Holbrooks Rd,...|ChIJe7EbHJDFsGoRA...|    206 Holbrooks Rd|-34.9228135|138.5484784|                1|    5032|    OK|      street_address|            null|\n",
      "|           ROOFTOP|8 Marion Rd, Unde...|ChIJV16rmoXFsGoRv...|       8A  Marion Rd|-34.9256164|138.5518486|                1|    5032|    OK|      street_address|            null|\n",
      "|           ROOFTOP|8 Marion Rd, Unde...|ChIJV16rmoXFsGoRv...|       8D  Marion Rd|-34.9256164|138.5518486|                1|    5032|    OK|      street_address|            null|\n",
      "|           ROOFTOP|23 Findon Rd, Woo...|ChIJEQPkAXHGsGoRG...|       23  Findon Rd|-34.8824023|138.5312475|                1|    5011|    OK|             premise|            null|\n",
      "|           ROOFTOP|8 Marion Rd, Unde...|ChIJV16rmoXFsGoRv...|       8K  Marion Rd|-34.9256164|138.5518486|                1|    5032|    OK|      street_address|            null|\n",
      "|RANGE_INTERPOLATED|20 Cross Rd, Urrb...|EicyMCBDcm9zcyBSZ...|        20  Cross Rd|-34.9639244|138.6389671|                1|    5064|    OK|      street_address|            null|\n",
      "|           ROOFTOP|22 Crittenden Rd,...|ChIJG7M28-XFsGoRA...|  22A  Crittenden Rd|-34.9035046|138.5449643|                1|    5023|    OK|      street_address|            null|\n",
      "|           ROOFTOP|180 Cross Rd, Mal...|ChIJwwjLTlrOsGoR1...|        180 Cross Rd| -34.965057|138.6109484|                1|    5061|    OK|      street_address|            null|\n",
      "|           ROOFTOP|8 Marion Rd, Unde...|ChIJV16rmoXFsGoRv...|       8C  Marion Rd|-34.9256164|138.5518486|                1|    5032|    OK|      street_address|            null|\n",
      "|           ROOFTOP|173 Cross Rd, Wes...|ChIJoeTUiLbPsGoRC...|        173 Cross Rd|-34.9666669|138.5929147|                1|    5041|    OK|      street_address|            null|\n",
      "|           ROOFTOP|13 Holbrooks Rd, ...|ChIJqehpgfrFsGoRn...|    13  Holbrooks Rd|-34.9061626|138.5509804|                1|    5025|    OK|      street_address|            null|\n",
      "|           ROOFTOP|218 Findon Rd, Fi...|ChIJ28FAGdvFsGoRZ...|       218 Findon Rd| -34.896977|138.5316035|                1|    5023|    OK|      street_address|            null|\n",
      "+------------------+--------------------+--------------------+--------------------+-----------+-----------+-----------------+--------+------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_geo_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def calc_dist(lat, long):\n",
    "    # define your distance calculation here\n",
    "    # for example, using the haversine formula\n",
    "    return distance\n",
    "\n",
    "calc_dist_udf = udf(calc_dist, DoubleType())\n",
    "\n",
    "out_geo_schema = out_geo_schema.withColumn(\"dist_from_centre\", calc_dist_udf(\"latitude\", \"longitude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[accuracy: string, formatted_address: string, google_place_id: string, input_string: string, latitude: double, longitude: double, number_of_results: int, postcode: string, status: string, type: string, dist_from_centre: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_geo_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "out_geo1 = out_geo_schema.withColumn(\"type\", when(out_geo_schema[\"type\"].isNull(), \"street_address\").otherwise(out_geo_schema[\"type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the withColumn() method to create a new column type in the out_geo DataFrame. We use the when() function to check if the value of type is null using the isNull() method. If it is null, we replace it with the string \"street_address\". Otherwise, we keep the existing value using the otherwise() method.\n",
    "\n",
    "\n",
    "Make sure to import the required modules at the beginning of your code before applying this operation to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- accuracy: string (nullable = true)\n",
      " |-- formatted_address: string (nullable = true)\n",
      " |-- google_place_id: string (nullable = true)\n",
      " |-- input_string: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- number_of_results: integer (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- dist_from_centre: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_geo_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to extract the last element of a comma-separated string\n",
    "extract_last = udf(lambda x: str(x).split(',')[-1], StringType())\n",
    "\n",
    "# Apply the UDF to the \"type\" column of the DataFrame\n",
    "out_geo = out_geo_schema.withColumn(\"type\", extract_last(\"type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first defines a User-Defined Function (UDF) using the udf() function from PySpark. The UDF takes a string as input, splits it by commas, and returns the last element of the resulting list as a string.\n",
    "\n",
    "Then, the withColumn() method is used to apply the UDF to the \"type\" column of the out_geo DataFrame, and assign the result back to the \"type\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnull, sum\n",
    "\n",
    "null_count = out_geo1.select(sum(isnull(col('type')).cast('integer')).alias('type')).collect()[0]['type']\n",
    "\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                type|\n",
      "+--------------------+\n",
      "|premise,street_ad...|\n",
      "|   country,political|\n",
      "|establishment,poi...|\n",
      "|          subpremise|\n",
      "|                null|\n",
      "|             premise|\n",
      "|establishment,poi...|\n",
      "|establishment,poi...|\n",
      "|administrative_ar...|\n",
      "|bar,establishment...|\n",
      "|establishment,poi...|\n",
      "|        intersection|\n",
      "|establishment,poi...|\n",
      "|establishment,gym...|\n",
      "|book_store,cafe,e...|\n",
      "|establishment,poi...|\n",
      "|establishment,fin...|\n",
      "|establishment,lod...|\n",
      "|establishment,lod...|\n",
      "|      street_address|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_types = out_geo_schema.select(\"type\").distinct()\n",
    "unique_types.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code selects only the \"type\" column from the out_geo DataFrame using the select() method, and then applies the distinct() method to get the unique values. The resulting unique_types DataFrame contains only one column with the unique values of the \"type\" column in out_geo.\n",
    "\n",
    "Finally, the show() method is used to display the contents of the unique_types DataFrame, which will output a table with one row for each unique value of the \"type\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the holiday_label function as a UDF\n",
    "def holiday_label(row):\n",
    "    if row == datetime.date(2013, 9, 1) :\n",
    "        return '1'\n",
    "    if row == datetime.date(2013, 10, 6) :\n",
    "        return '1'\n",
    "    if row == datetime.date(2013, 12, 22) :\n",
    "        return '2'\n",
    "    if row == datetime.date(2013, 12, 29):\n",
    "        return '1'\n",
    "    if row == datetime.date(2014, 1, 26):\n",
    "        return '1'\n",
    "    if row == datetime.date(2014, 3, 9):\n",
    "        return '1'\n",
    "    if row == datetime.date(2014, 4, 13) :\n",
    "        return '2'\n",
    "    if row == datetime.date(2014, 4, 20):\n",
    "        return '2'\n",
    "    if row == datetime.date(2014, 6, 8):\n",
    "        return '1'\n",
    "    return '0'\n",
    "\n",
    "holiday_label_udf = udf(holiday_label, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[TripID: int, RouteID: string, StopID: int, StopName: string, WeekBegining: date, NumberOfBoardings: int, formatted_address: string, latitude: double, longitude: double, postcode: string, type: string, route_desc: string, dist_from_centre: double, holiday_label: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, date_format\n",
    "\n",
    "df_with_schema.withColumn(\"WeekBegining\", to_date(date_format(\"WeekBegining\", \"dd/MM/yyyy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "|TripID|RouteID|StopID|            StopName|WeekBegining|NumberOfBoardings|   formatted_address|           latitude|         longitude|postcode|           type|          route_desc|  dist_from_centre|holiday_label|\n",
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "| 23631|    100| 14156|        181 Cross Rd|  2013-06-30|                1|181 Cross Rd, Wes...|        -34.9666565|       138.5921483|    5041| street_address|via Woodville Roa...| 5.180961380506887|            0|\n",
      "| 23631|    100| 14144|        177 Cross Rd|  2013-06-30|                1|177 Cross Rd, Wes...|        -34.9666071|       138.5923007|    5041| street_address|via Woodville Roa...| 5.172525305414759|            0|\n",
      "| 23632|    100| 14132|        175 Cross Rd|  2013-06-30|                1|175 Cross Rd, Wes...|-34.966757799999996|       138.5927151|    5041| street_address|via Woodville Roa...| 5.180708757110343|            0|\n",
      "| 23633|    100| 12266|Zone A Arndale In...|  2013-06-30|                2|Zone A Arndale In...|          -34.87516|        138.551628|    5009|transit_station|via Woodville Roa...| 7.057549100180063|            0|\n",
      "| 23633|    100| 14147|        178 Cross Rd|  2013-06-30|                1|178 Cross Rd, Mal...|        -34.9649596|        138.611477|    5061| street_address|via Woodville Roa...| 4.900099234428335|            0|\n",
      "| 23634|    100| 13907|       9A  Marion Rd|  2013-06-30|                1|9 Marion Rd, Torr...|        -34.9256552|       138.5523584|    5031| street_address|via Woodville Roa...| 4.807796587273396|            0|\n",
      "| 23634|    100| 14132|        175 Cross Rd|  2013-06-30|                1|175 Cross Rd, Wes...|-34.966757799999996|       138.5927151|    5041| street_address|via Woodville Roa...| 5.180708757110343|            0|\n",
      "| 23634|    100| 13335|    9A  Holbrooks Rd|  2013-06-30|                1|9 Holbrooks Rd, F...|        -34.9058902|       138.5511989|    5025| street_address|via Woodville Roa...| 5.178866284604099|            0|\n",
      "| 23634|    100| 13875|        9  Marion Rd|  2013-06-30|                1|9 Marion Rd, Torr...|        -34.9256552|       138.5523584|    5031| street_address|via Woodville Roa...| 4.807796587273396|            0|\n",
      "| 23634|    100| 13045|    206 Holbrooks Rd|  2013-06-30|                1|206 Holbrooks Rd,...|-34.922813500000004|       138.5484784|    5032| street_address|via Woodville Roa...| 5.139625006923029|            0|\n",
      "| 23635|    100| 13335|    9A  Holbrooks Rd|  2013-06-30|                1|9 Holbrooks Rd, F...|        -34.9058902|       138.5511989|    5025| street_address|via Woodville Roa...| 5.178866284604099|            0|\n",
      "| 23635|    100| 13383|       8A  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 13586|       8D  Marion Rd|  2013-06-30|                2|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 12726|       23  Findon Rd|  2013-06-30|                1|23 Findon Rd, Woo...|-34.882402299999995|       138.5312475|    5011|        premise|via Woodville Roa...| 7.980582863248004|            0|\n",
      "| 23635|    100| 13813|       8K  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23635|    100| 14062|        20  Cross Rd|  2013-06-30|                1|20 Cross Rd, Urrb...|        -34.9639244|138.63896709999997|    5064| street_address|via Woodville Roa...| 5.677892683112502|            0|\n",
      "| 23636|    100| 12780|  22A  Crittenden Rd|  2013-06-30|                1|22 Crittenden Rd,...|-34.903504600000005|       138.5449643|    5023| street_address|via Woodville Roa...| 5.803639984741648|            0|\n",
      "| 23636|    100| 13383|       8A  Marion Rd|  2013-06-30|                1|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "| 23636|    100| 14154|        180 Cross Rd|  2013-06-30|                2|180 Cross Rd, Mal...|         -34.965057|       138.6109484|    5061| street_address|via Woodville Roa...|4.9051084510922465|            0|\n",
      "| 23636|    100| 13524|       8C  Marion Rd|  2013-06-30|                3|8 Marion Rd, Unde...|-34.925616399999996|       138.5518486|    5032| street_address|via Woodville Roa...| 4.853616975966056|            0|\n",
      "+------+-------+------+--------------------+------------+-----------------+--------------------+-------------------+------------------+--------+---------------+--------------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df_with_schema.withColumn(\"WeekBegining\", to_date(\"WeekBegining\")) \\\n",
    "       .withColumn(\"holiday_label\", holiday_label_udf(\"WeekBegining\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
