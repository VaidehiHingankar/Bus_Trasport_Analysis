airflow installation
---------------------
sudo apt-get update
sudo apt-get install python3-pip python3-dev
sudo apt-get install libpq-dev

export SLUGIFY_USES_TEXT_UNIDECODE=yes
pip install apache-airflow

airflow initdb

cd ~/airflow
mkdir dags logs
echo -e "[core]\ndags_folder=/home/ubh01/airflow/dags\nbase_log_folder=/home/ubh01/airflow/logs" > airflow.cfg

export AIRFLOW_HOME=~/airflow
export PYTHONPATH=$PYTHONPATH:$AIRFLOW_HOME

airflow webserver -p 8080 -D
airflow scheduler -D

Access the Airflow web UI - http://localhost:8080
-----------------------------------------------------------------------------------------------

airflow script
-----------------
import datetime as dt
from datetime import timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator

default_args = {
    'owner': 'myusername',
    'depends_on_past': False,
    'start_date': dt.datetime(2023, 3, 15),
    'retries': 1,
    'retry_delay':timedelta(minutes=1),
    'timezone':'Asia/Kolkata'
}

dag = DAG('run_pyspark_script_daily1',
          default_args=default_args,
          schedule_interval='21 10 * * *',
          catchup=False)

task = BashOperator(task_id='run_pyspark_script',
                    bash_command='spark-submit /home/ubh01/spark-3.0.3-bin-hadoop2.7/bin/BusTransportScript.py',
                    dag=dag)
task